import os
import json
import time
from datetime import datetime
from typing import Dict, Any, List

from langchain_community.document_loaders import WebBaseLoader
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# å¼•å…¥æ ¸å¿ƒæ¨¡å—
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import Config
from .rag_engine import rag_engine
from .pdf_extractor import pdf_extractor

class PolicyAnalyzer:
    """
    æ ¸å¿ƒåˆ†æå¼•æ“ (RAG å¢å¼ºç‰ˆ + PDF æ”¯æŒ)ï¼š
    1. æŠ“å– URL å†…å®¹
    2. æ£€æµ‹å¹¶æå–åµŒå…¥çš„ PDF æ–‡ä»¶
    3. ä½¿ç”¨ RAG å¼•æ“è¿›è¡Œè¯­ä¹‰åˆ‡ç‰‡ä¸ç´¢å¼•
    4. æ£€ç´¢å…³é”®è¯åŸæ–‡ä¾æ®
    5. è°ƒç”¨ LLM è¿›è¡Œæ·±åº¦æŠ•ç ”åˆ†æ
    6. è¾“å‡ºç»“æ„åŒ– JSON (å« PDF ä¸‹è½½é“¾æ¥)
    """

    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=Config.DASHSCOPE_API_KEY,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            model=Config.MODEL_NAME,
            temperature=0.1,  # é™ä½æ¸©åº¦ä»¥å‡å°‘å¹»è§‰é£é™©
            model_kwargs={
                "response_format": {"type": "json_object"}
            }
        )

    def scrape_url(self, url: str) -> str:
        """ç½‘é¡µæŠ“å–"""
        print(f"ğŸ•·ï¸ æ­£åœ¨è¯»å–ç½‘é¡µå†…å®¹: {url} ...")
        try:
            loader = WebBaseLoader(url)
            loader.requests_kwargs = {'verify': False, 'timeout': 15}
            docs = loader.load()
            content = "\n\n".join([d.page_content for d in docs])
            return content[:25000] # æ‰©å¤§æŠ“å–èŒƒå›´ï¼Œäº¤ç»™ RAG å¤„ç†
        except Exception as e:
            print(f"âŒ ç½‘é¡µæŠ“å–å¤±è´¥: {e}")
            return ""

    def analyze(self, policy_data: Dict[str, Any], stage_callback=None) -> Dict[str, Any]:
        """
        æ ¸å¿ƒåˆ†æé€»è¾‘ (æ”¯æŒ RAGã€PDFè§£æ å’Œ é˜¶æ®µå›è°ƒ)
        """
        url = policy_data.get('link')
        pdf_download_url = None
        content_source = "webpage"  # Debug: è®°å½•å†…å®¹æ¥æº
        
        # Step 1: å…ˆå°è¯•æå– PDFï¼ˆæ”¿ç­–åŸæ–‡é€šå¸¸åœ¨ PDF ä¸­ï¼‰
        if stage_callback: stage_callback("ğŸ“„ æ­£åœ¨æ£€æµ‹ PDF æ”¿ç­–åŸæ–‡...", 10)
        pdf_result = pdf_extractor.extract_and_parse(url)
        
        raw_text = ""
        
        # ä¼˜å…ˆä½¿ç”¨ PDF å†…å®¹ï¼ˆåªè¦æœ‰å®è´¨å†…å®¹ï¼‰
        if pdf_result["pdf_content"] and len(pdf_result["pdf_content"]) > 500:
            print(f"âœ… æ£€æµ‹åˆ° PDF æ”¿ç­–åŸæ–‡ï¼Œä¼˜å…ˆä½¿ç”¨ PDF å†…å®¹ ({len(pdf_result['pdf_content'])} å­—)")
            raw_text = pdf_result["pdf_content"]
            pdf_download_url = pdf_result["source_pdf_url"]
            content_source = "pdf"
        else:
            # Fallback: æŠ“å–ç½‘é¡µå†…å®¹
            if stage_callback: stage_callback("ğŸ“– æœªæ‰¾åˆ° PDFï¼Œæ­£åœ¨è¯»å–ç½‘é¡µå†…å®¹...", 20)
            raw_text = self.scrape_url(url)
            content_source = "webpage"
            # è®°å½• PDF æå–å¤±è´¥çš„è¯Šæ–­ä¿¡æ¯
            pdf_extraction_error = pdf_result.get("error", "æœªçŸ¥åŸå› ")
            pdf_links_found = pdf_result.get("pdf_links", [])
            if pdf_links_found:
                pdf_download_url = pdf_links_found[0]["url"]
                print(f"âš ï¸ å‘ç° {len(pdf_links_found)} ä¸ª PDF é“¾æ¥ä½†è§£æå¤±è´¥: {pdf_extraction_error}")
                print(f"   é¦–ä¸ªé“¾æ¥: {pdf_download_url[:80]}...")
            else:
                print(f"âš ï¸ æœªåœ¨é¡µé¢ä¸­å‘ç°ä»»ä½• PDF é“¾æ¥")
        
        if not raw_text:
            return {"error": "æ— æ³•è·å–ç½‘é¡µæˆ–PDFå†…å®¹"}

        # Step 2: RAG ç´¢å¼•
        if stage_callback: stage_callback("ğŸ§  æ­£åœ¨æ„å»ºè¯­ä¹‰ç´¢å¼• (RAG)...", 30)
        vector_store = rag_engine.create_index(raw_text)
        
        # Step 3: åŸæ–‡æ£€ç´¢
        if stage_callback: stage_callback("ğŸ” æ­£åœ¨æ£€ç´¢åŸæ–‡å…³é”®æ¡æ¬¾...", 50)
        
        # ä¼˜åŒ–æ£€ç´¢ queryï¼šè¦†ç›–æ›´å¤šæ”¿ç­–é‡ç‚¹åœºæ™¯
        search_queries = [
            "æ–°å¢æ¡æ¬¾å’Œè§„å®š",           # æ–°ç›‘ç®¡ç±»
            "ä¿®è®¢å†…å®¹å’Œè°ƒæ•´å¹…åº¦",       # ä¿®è®¢ç±»
            "æ•°é‡é™åˆ¶ã€æ¯”ä¾‹è¦æ±‚ã€é‡‘é¢ä¸Šé™",  # æ•°å­—ç»†èŠ‚
            "ç”Ÿæ•ˆæ—¥æœŸã€è¿‡æ¸¡æœŸã€å®æ–½æ—¶é—´",   # æ—¶é—´èŠ‚ç‚¹
            "è¿è§„å¤„ç½šã€æ³•å¾‹è´£ä»»ã€ç›‘ç®¡æªæ–½",  # åˆè§„é‡ç‚¹
            "å…¬å‹ŸåŸºé‡‘ã€æŒ‡æ•°åŸºé‡‘ã€ETFç›¸å…³è§„å®š",  # è¡Œä¸šç›¸å…³
            "ä¿¡æ¯æŠ«éœ²ã€æŠ¥å‘Šä¹‰åŠ¡ã€å¤‡æ¡ˆè¦æ±‚"   # åˆè§„ä¹‰åŠ¡
        ]
        original_citations = rag_engine.get_context_for_analysis(vector_store, search_queries, k=4)
        
        # æ‰“å°æ£€ç´¢ç»“æœç”¨äºè°ƒè¯•
        print(f"ğŸ” RAG æ£€ç´¢ç»“æœ: {len(original_citations)} å­—ç¬¦")

        # Step 4: LLM åˆ†æ
        if stage_callback: stage_callback("ğŸ“Š æ­£åœ¨è°ƒç”¨ Qwen-Max è¿›è¡ŒæŠ•ç ”æ·±åº¦åˆ†æ...", 70)
        
        system_prompt = """ä½ æ˜¯ã€æ˜“æ–¹è¾¾åŸºé‡‘é¦–å¸­æ”¿ç­–åˆ†æå¸ˆã€‘ï¼Œè¯·ä¸¥æ ¼åŸºäºæ”¿ç­–åŸæ–‡æ’°å†™ä¸“ä¸šæŠ•ç ”æŠ¥å‘Šã€‚

ã€é‡‘èè¡Œä¸šç®€ç§°å¯¹ç…§è¡¨ã€‘(åˆ†ææ—¶éœ€ç†è§£è¿™äº›å¯¹ç­‰æ¦‚å¿µ)
- å…¬å‹ŸåŸºé‡‘ = å…¬å¼€å‹Ÿé›†è¯åˆ¸æŠ•èµ„åŸºé‡‘
- ç§å‹ŸåŸºé‡‘ = ç§å‹ŸæŠ•èµ„åŸºé‡‘
- ETF = äº¤æ˜“å‹å¼€æ”¾å¼æŒ‡æ•°åŸºé‡‘
- LOF = ä¸Šå¸‚å¼€æ”¾å¼åŸºé‡‘
- QDII = åˆæ ¼å¢ƒå†…æœºæ„æŠ•èµ„è€…
- FOF = åŸºé‡‘ä¸­åŸºé‡‘
- æŒ‡æ•°åŸºé‡‘ = æŒ‡æ•°å‹è¯åˆ¸æŠ•èµ„åŸºé‡‘
- è¯åˆ¸å…¬å¸ = è¯åˆ¸ç»è¥æœºæ„
- åŸºé‡‘å…¬å¸ = åŸºé‡‘ç®¡ç†å…¬å¸/åŸºé‡‘ç®¡ç†äºº
- æ‰˜ç®¡è¡Œ = åŸºé‡‘æ‰˜ç®¡äºº/æ‰˜ç®¡é“¶è¡Œ
- å‡æŒæ–°è§„ = è‚¡ä»½å‡æŒè§„åˆ™/å‡æŒç®¡ç†åŠæ³•

ã€æ”¿ç­–åˆ†ç±»ä¸é‡ç‚¹è¯†åˆ«ã€‘
è¯·å…ˆåˆ¤æ–­æ”¿ç­–ç±»å‹ï¼Œå†é‡ç‚¹å…³æ³¨å¯¹åº”å†…å®¹ï¼š

1. **ä¿®è®¢ç±»æ”¿ç­–**ï¼šé‡ç‚¹å…³æ³¨
   - æ•°é‡/æ¯”ä¾‹çš„å¤§å¹…è°ƒæ•´ï¼ˆå¦‚ä»X%è°ƒæ•´ä¸ºY%ï¼‰
   - æ–°å¢æˆ–åˆ é™¤çš„å…³é”®æ¡æ¬¾
   - é€‚ç”¨èŒƒå›´çš„æ‰©å¤§æˆ–ç¼©å°

2. **è¡Œä¸šè§„å®šç±»æ”¿ç­–**ï¼šé‡ç‚¹å…³æ³¨
   - æ–°å¢çš„è¡Œä¸šè§„èŒƒå’Œæ ‡å‡†
   - å¯¹ç°æœ‰è§„åˆ™çš„é‡å¤§è°ƒæ•´
   - æ–°çš„åˆè§„ä¹‰åŠ¡å’ŒæŠ¥å‘Šè¦æ±‚

3. **æ–°ç›‘ç®¡ç±»æ”¿ç­–**ï¼šé‡ç‚¹å…³æ³¨
   - ç›‘ç®¡è§„åˆ™æ˜¯æ”¶ç´§è¿˜æ˜¯æ”¾æ¾
   - æ–°å¢çš„é™åˆ¶æ€§è§„å®š
   - æ–°çš„å¤„ç½šæ¡æ¬¾å’Œæ³•å¾‹è´£ä»»

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. **ä¸¥ç¦è™šæ„**ï¼šæ‰€æœ‰æ•°å­—ã€æ—¥æœŸã€ç™¾åˆ†æ¯”ã€æ¡æ¬¾ç¼–å·å¿…é¡»ç›´æ¥æ¥è‡ªåŸæ–‡ï¼Œä¸å¯æ¨æµ‹æˆ–ç¼–é€ 
2. **åŸæ–‡é”šå®š**ï¼šæ¯ä¸ªæ ¸å¿ƒè§‚ç‚¹å¿…é¡»æ ‡æ³¨åŸæ–‡å‡ºå¤„ï¼Œå¦‚"æ ¹æ®ç¬¬Xæ¡..."æˆ–ç›´æ¥å¼•ç”¨åŸæ–‡
3. **ä¸ç¡®å®šæ€§æ ‡æ³¨**ï¼šå¦‚åŸæ–‡æœªæ˜ç¡®æŸä¿¡æ¯ï¼Œéœ€æ˜ç¡®æ³¨æ˜"åŸæ–‡æœªæ˜ç¡®è¯´æ˜"
4. **åŒºåˆ†çŸ­æœŸ/é•¿æœŸå½±å“**

ã€æŠ¥å‘Šç»“æ„ (å…±çº¦1800å­—)ã€‘
1. **æ‘˜è¦** (250å­—): æ”¿ç­–èƒŒæ™¯ã€æ ¸å¿ƒå˜åŒ–ã€ä¸»è¦å½±å“
2. **æ”¿ç­–è¦ç‚¹** (250å­—): ç›‘ç®¡è§„å®šã€åˆè§„è¦æ±‚ã€å…³é”®æ¡æ¬¾
3. **åŸæ–‡æ‘˜å½•** (200å­—): é€‰å–æœ€å…³é”®çš„2-3æ¡åŸæ–‡å¹¶ç®€è¦è§£è¯»
4. **å¸‚åœºå½±å“** (400å­—): çŸ­æœŸå†²å‡»(3-6æœˆ) + é•¿æœŸè¶‹åŠ¿(1-3å¹´)
5. **æ˜“æ–¹è¾¾è¡ŒåŠ¨å»ºè®®** (400å­—): äº§å“ç­–ç•¥ã€ä¸šåŠ¡è°ƒæ•´ã€èµ„æºé…ç½®
6. **é£é™©æç¤º** (100å­—): éœ€å…³æ³¨çš„ä¸ç¡®å®šæ€§

ã€è¾“å‡ºJSONæ ¼å¼ã€‘
{{
  "selected_policy": {{"title": "{title}", "issuer": "{source}", "publish_date": "{date}", "url": "{url}"}},
  "chat_bullets": ["æ ¸å¿ƒè§‚ç‚¹1(å«åŸæ–‡å¼•ç”¨)", "æ ¸å¿ƒè§‚ç‚¹2", "æ ¸å¿ƒè§‚ç‚¹3", "æ ¸å¿ƒè§‚ç‚¹4", "æ ¸å¿ƒè§‚ç‚¹5", "æ ¸å¿ƒè§‚ç‚¹6"],
  "docx_content": {{
    "æ‘˜è¦": ["æ®µè½1", "æ®µè½2"],
    "æ”¿ç­–è¦ç‚¹": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
    "åŸæ–‡æ‘˜å½•": ["åŸæ–‡1åŠè§£è¯»", "åŸæ–‡2åŠè§£è¯»"],
    "å¸‚åœºå½±å“": ["çŸ­æœŸå½±å“", "é•¿æœŸå½±å“"],
    "æ˜“æ–¹è¾¾è¡ŒåŠ¨å»ºè®®": ["äº§å“ç­–ç•¥", "ä¸šåŠ¡è°ƒæ•´å»ºè®®"],
    "é£é™©æç¤º": ["é£é™©ç‚¹"]
  }}
}}
"""
        
        user_prompt = """è¯·åŸºäºä»¥ä¸‹æ”¿ç­–å†…å®¹æ’°å†™çº¦1800å­—çš„ä¸“ä¸šåˆ†ææŠ¥å‘Šã€‚

ã€RAGæ£€ç´¢åˆ°çš„å…³é”®åŸæ–‡ã€‘(è¯·ä¼˜å…ˆå¼•ç”¨è¿™äº›æ¡æ¬¾)
{citations}

ã€æ”¿ç­–å…¨æ–‡å‚è€ƒã€‘
{content}

âš ï¸ ã€å¹»è§‰é˜²èŒƒæœºåˆ¶ - åŠ¡å¿…ä¸¥æ ¼éµå®ˆã€‘ï¼š
- å¦‚æœåŸæ–‡ä¸­æ²¡æœ‰å…·ä½“æ•°å­—ï¼Œä¸¥ç¦ç¼–é€ ä»»ä½•æ•°å­—ï¼ˆå¦‚ç™¾åˆ†æ¯”ã€é‡‘é¢ã€å¤©æ•°ã€æ¯”ä¾‹ï¼‰
- å¦‚æœæ— æ³•ç¡®è®¤æŸæ¡æ¬¾çš„å…·ä½“å†…å®¹ï¼Œè¯·æ˜ç¡®å†™å‡º"åŸæ–‡æœªæ˜ç¡®è§„å®š"æˆ–"éœ€è¿›ä¸€æ­¥ç¡®è®¤"
- æ¯ä¸ª"chat_bullets"å¿…é¡»é™„å¸¦ä¸€ä¸ªå¯éªŒè¯çš„åŸæ–‡ç‰‡æ®µä½œä¸ºä¾æ®
- ç¦æ­¢ä½¿ç”¨"æ®æ‚‰"ã€"é¢„è®¡"ã€"å¯èƒ½ä¼š"ç­‰æ¨æµ‹æ€§è¡¨è¿°ï¼Œé™¤éåŸæ–‡å¦‚æ­¤è¡¨è¿°
- "åŸæ–‡æ‘˜å½•"éƒ¨åˆ†å¿…é¡»æ˜¯æ”¿ç­–æ–‡ä»¶ä¸­çš„çœŸå®åŸå¥ï¼Œä¸å¯æ”¹å†™æˆ–æ€»ç»“

ğŸ“ è¾“å‡ºè¦æ±‚ï¼š
- chat_bullets æ¯æ¡éœ€ç®€æ´æœ‰åŠ›ï¼Œçº¦30-50å­—ï¼Œå¿…é¡»åŒ…å«åŸæ–‡ä¾æ®
- å¸‚åœºå½±å“éœ€åŒºåˆ†çŸ­æœŸ(3-6æœˆ)å’Œé•¿æœŸ(1-3å¹´)
- æ˜“æ–¹è¾¾å»ºè®®éœ€å…·ä½“å¯æ“ä½œï¼Œæ¶µç›–äº§å“ã€ä¸šåŠ¡ã€èµ„æºä¸‰æ–¹é¢
- ä¸¥æ ¼è¾“å‡ºJSONæ ¼å¼ï¼Œå‹¿æ·»åŠ markdownæ ‡è®°
"""

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("user", user_prompt)
        ])

        chain = prompt | self.llm | StrOutputParser()
        
        try:
            response_str = chain.invoke({
                "title": policy_data.get('title'),
                "source": policy_data.get('source'),
                "date": policy_data.get('date'),
                "url": url,
                "citations": original_citations,
                "content": raw_text[:12000] # å‘é€éƒ¨åˆ†å…¨æ–‡ä½œä¸ºèƒŒæ™¯
            })
            
            if stage_callback: stage_callback("ğŸ“ æ­£åœ¨æ•´ç†è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š...", 90)
            result = json.loads(response_str)
            
            # æ³¨å…¥ PDF ä¸‹è½½é“¾æ¥
            if pdf_download_url:
                result["pdf_download_url"] = pdf_download_url
            
            # æ³¨å…¥åŸå§‹é‡‡é›†å¿«ç…§ (Debug ç”¨)
            result["debug_content_source"] = content_source  # "pdf" æˆ– "webpage"
            result["debug_raw_text"] = raw_text[:2000] + ("..." if len(raw_text) > 2000 else "")
            result["debug_citations"] = original_citations
            result["debug_pdf_links"] = pdf_result.get("pdf_links", [])
            result["debug_pdf_error"] = pdf_result.get("error", None)
                
            return result
            
        except Exception as e:
            print(f"âŒ LLM åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}