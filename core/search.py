import requests
import json
from typing import List, Dict, Optional
import sys
import os

# ç¡®ä¿èƒ½å¯¼å…¥åŒçº§æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from config import Config
except ImportError:
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from config import Config

class PolicySearcher:
    """
    è´Ÿè´£è”ç½‘æ£€ç´¢ (Stage 1: Recall)
    """
    
    @staticmethod
    def search(query: str, num_results: int = 50, 
               source_preference: str = "all", 
               time_range: Optional[str] = None,
               raw_query: Optional[str] = None) -> List[Dict]:
        """
        æ‰§è¡ŒåŒå‘é‡æœç´¢ (Stage 1: Multi-Vector Recall)
        1. åŸå§‹æœç´¢: ä¿¡ä»» Google çš„åŸç”Ÿç†è§£
        2. ç²¾ç‚¼æœç´¢: ä½¿ç”¨ AI å¤„ç†åçš„å…³é”®è¯ + ç«™ç‚¹é™åˆ¶
        """
        queries_to_run = []
        
        # 1. åŸå§‹æŸ¥è¯¢ (Raw)
        if raw_query:
            queries_to_run.append({"q": raw_query, "type": "raw"})
        
        # 2. ç²¾ç‚¼æŸ¥è¯¢ (Refined)
        refined_q = query
        if source_preference == "gov":
            refined_q += " (site:gov.cn OR site:amac.org.cn OR site:sse.com.cn OR site:szse.cn OR site:bse.cn)"
        if time_range:
            refined_q += f" {time_range}"
        queries_to_run.append({"q": refined_q, "type": "refined"})

        all_candidates = {} # url -> candidate_dict

        for q_item in queries_to_run:
            print(f"ğŸ” [SerpApi] æ­£åœ¨è¿›è¡Œ{q_item['type']}æ£€ç´¢: {q_item['q']} ...")
            
            url = "https://serpapi.com/search"
            params = {
                "engine": "google",
                "q": q_item['q'],
                "api_key": Config.SERPER_API_KEY,
                "gl": "cn",
                "hl": "zh-cn",
                "num": num_results
            }

            try:
                response = requests.get(url, params=params, timeout=15)
                response.raise_for_status()
                data = response.json()
                raw_results = data.get("organic_results", [])
                
                for idx, item in enumerate(raw_results):
                    link = item.get("link", "")
                    if not link: continue
                    
                    # åŸå§‹æ’å (ä»1å¼€å§‹)
                    rank = item.get("position", idx + 1)
                    
                    source_info = item.get("source", "")
                    if not source_info and "displayed_link" in item:
                        source_info = item["displayed_link"]

                    # å¦‚æœ URL å·²å­˜åœ¨ï¼Œä¿ç•™æ›´å¥½çš„æ’å
                    if link in all_candidates:
                        if rank < all_candidates[link]['google_rank']:
                            all_candidates[link]['google_rank'] = rank
                    else:
                        all_candidates[link] = {
                            "title": item.get("title", ""),
                            "link": link,
                            "snippet": item.get("snippet", ""),
                            "date": item.get("date", ""),
                            "source": source_info,
                            "google_rank": rank, # è®°å½• Google åŸå§‹æ’å
                            "search_type": q_item['type']
                        }
            except Exception as e:
                print(f"âŒ æœç´¢ API è°ƒç”¨å¤±è´¥ [{q_item['type']}]: {e}")

        # è½¬ä¸ºåˆ—è¡¨å¹¶è¾“å‡º
        unique_candidates = list(all_candidates.values())
        print(f"ğŸ“¥ [SerpApi] æ··åˆå¬å›æ€»é‡: {len(unique_candidates)} æ¡")
        
        return unique_candidates
