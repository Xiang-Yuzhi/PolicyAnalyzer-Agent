"""
RAG Engine: æ£€ç´¢å¢å¼ºç”Ÿæˆæ ¸å¿ƒ

è´Ÿè´£æ”¿ç­–æ–‡æ¡£çš„åˆ‡ç‰‡ã€å‘é‡åŒ–å­˜å‚¨åŠè¯­ä¹‰æ£€ç´¢ï¼Œæ”¯æŒç²¾å‡†åŸæ–‡å¼•ç”¨ã€‚
"""

import os
from typing import List, Dict, Any
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# å¼•å…¥é…ç½®
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import Config

class RAGEngine:
    """
    RAG å¼•æ“ï¼šå¤„ç†æ–‡æ¡£ç´¢å¼•ä¸æ£€ç´¢
    """
    
    def __init__(self):
        # åˆå§‹åŒ– Embedding æ¨¡å‹ (é€‚é… DashScope å…¼å®¹æ¨¡å¼)
        self.embeddings = OpenAIEmbeddings(
            api_key=Config.DASHSCOPE_API_KEY,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            model="text-embedding-v3"
        )
        
        # å®šä¹‰åˆ‡ç‰‡å™¨ (å¢å¤§åˆ‡ç‰‡é•¿åº¦ä¿ç•™æ›´å®Œæ•´ä¸Šä¸‹æ–‡)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # å¢å¤§åˆ‡ç‰‡ä»¥ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡
            chunk_overlap=200,  # å¢åŠ é‡å é˜²æ­¢å…³é”®ä¿¡æ¯è¢«åˆ‡æ–­
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
        )

    def create_index(self, text: str):
        """
        å¯¹å•ä»½æ”¿ç­–æ–‡æœ¬å»ºç«‹å‘é‡ç´¢å¼•
        """
        if not text:
            print("âš ï¸ RAG: è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡ç´¢å¼•åˆ›å»º")
            return None
        
        # 1. æ–‡æœ¬åˆ‡ç‰‡
        chunks = self.text_splitter.split_text(text)
        print(f"ğŸ“š RAG: æ–‡æœ¬åˆ‡ç‰‡å®Œæˆï¼Œå…± {len(chunks)} ä¸ªç‰‡æ®µ (åŸæ–‡ {len(text)} å­—)")
        
        if not chunks:
            print("âš ï¸ RAG: åˆ‡ç‰‡ç»“æœä¸ºç©º")
            return None
        
        # 2. å»ºç«‹å‘é‡åº“
        try:
            vector_store = FAISS.from_texts(chunks, self.embeddings)
            print(f"âœ… RAG: å‘é‡ç´¢å¼•åˆ›å»ºæˆåŠŸ")
            return vector_store
        except Exception as e:
            print(f"âŒ RAG: å»ºç«‹å‘é‡ç´¢å¼•å¤±è´¥: {e}")
            return None

    def retrieve_relevant_chunks(self, vector_store, query: str, k: int = 5) -> List[str]:
        """
        ä»å‘é‡åº“ä¸­æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æœ¬å—
        """
        if not vector_store:
            return []
        
        try:
            docs = vector_store.similarity_search(query, k=k)
            return [doc.page_content for doc in docs]
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return []

    def get_context_for_analysis(self, vector_store, queries: List[str], k: int = 3) -> str:
        """
        ä¸ºå¤šä¸ªåˆ†æç»´åº¦è·å–ç»¼åˆä¸Šä¸‹æ–‡
        
        Args:
            k: æ¯ä¸ªqueryæ£€ç´¢çš„æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤3æ¡
        """
        if not vector_store:
            print("âš ï¸ RAG: vector_store ä¸ºç©ºï¼Œæ— æ³•æ£€ç´¢")
            return ""
        
        all_chunks = []
        for q in queries:
            chunks = self.retrieve_relevant_chunks(vector_store, q, k=k)
            print(f"  ğŸ” Query '{q[:20]}...' -> æ£€ç´¢åˆ° {len(chunks)} ä¸ªç‰‡æ®µ")
            all_chunks.extend(chunks)
        
        # å»é‡å¹¶åˆå¹¶
        unique_chunks = list(set(all_chunks))
        result = "\n---\n".join(unique_chunks)
        print(f"ğŸ“Š RAG æ£€ç´¢æ±‡æ€»: æ€» {len(all_chunks)} ä¸ªç‰‡æ®µ, å»é‡å {len(unique_chunks)} ä¸ª, å…± {len(result)} å­—ç¬¦")
        return result

# å•ä¾‹æ¨¡å¼ä¾›å¤–éƒ¨è°ƒç”¨
rag_engine = RAGEngine()
