"""
PDF Extractor: ä»ç½‘é¡µä¸­æå–å¹¶è§£æ PDF æ–‡ä»¶

åŠŸèƒ½ï¼š
1. ä»ç½‘é¡µå†…å®¹ä¸­æå– PDF ä¸‹è½½é“¾æ¥
2. ä¸‹è½½ PDF æ–‡ä»¶å¹¶æå–å…¨æ–‡å†…å®¹
3. æ”¯æŒåµŒå…¥å¼ PDF (embed/iframe) å’Œç›´æ¥é“¾æ¥
"""

import re
import requests
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, urlparse

# PDF è§£æ
try:
    import fitz  # PyMuPDF
    HAS_PYMUPDF = True
except ImportError:
    HAS_PYMUPDF = False
    print("âš ï¸ PyMuPDF æœªå®‰è£…ï¼Œå°†æ— æ³•è§£æ PDF å†…å®¹")

# HTML è§£æ
try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False
    print("âš ï¸ BeautifulSoup æœªå®‰è£…ï¼Œå°†æ— æ³•æå– PDF é“¾æ¥")


class PDFExtractor:
    """PDF æå–ä¸è§£æå™¨"""
    
    # å¸¸è§çš„æ”¿ç­– PDF æ–‡ä»¶åæ¨¡å¼
    PDF_PATTERNS = [
        r'\.pdf$',
        r'\.PDF$',
        r'/download/',
        r'/attachment/',
        r'/file/',
    ]
    
    # è¯·æ±‚å¤´æ¨¡æ‹Ÿæµè§ˆå™¨
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    }
    
    @staticmethod
    def extract_pdf_links(page_url: str, html_content: Optional[str] = None) -> List[Dict[str, str]]:
        """
        ä»ç½‘é¡µä¸­æå– PDF ä¸‹è½½é“¾æ¥
        
        Returns:
            [{"url": "å®Œæ•´PDFé“¾æ¥", "title": "é“¾æ¥æ–‡æœ¬æˆ–æ–‡ä»¶å"}]
        """
        if not HAS_BS4:
            return []
        
        pdf_links = []
        
        try:
            if not html_content:
                response = requests.get(page_url, headers=PDFExtractor.HEADERS, timeout=15, verify=False)
                html_content = response.text
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æ–¹å¼1: ç›´æ¥çš„ <a href="xxx.pdf"> é“¾æ¥
            for a in soup.find_all('a', href=True):
                href = a.get('href', '')
                if re.search(r'\.pdf', href, re.I):
                    full_url = urljoin(page_url, href)
                    title = a.get_text(strip=True) or PDFExtractor._extract_filename(full_url)
                    pdf_links.append({"url": full_url, "title": title})
            
            # æ–¹å¼2: åµŒå…¥çš„ <embed> æˆ– <iframe> æ ‡ç­¾
            for tag in soup.find_all(['embed', 'iframe', 'object']):
                src = tag.get('src') or tag.get('data', '')
                if src and '.pdf' in src.lower():
                    full_url = urljoin(page_url, src)
                    pdf_links.append({"url": full_url, "title": "åµŒå…¥å¼PDFæ–‡æ¡£"})
            
            # æ–¹å¼3: JavaScript åŠ¨æ€åŠ è½½çš„é“¾æ¥ (ç®€å•æ¨¡å¼åŒ¹é…)
            scripts = soup.find_all('script')
            for script in scripts:
                script_text = script.string or ''
                pdf_matches = re.findall(r'["\']([^"\']*\.pdf[^"\']*)["\']', script_text, re.I)
                for match in pdf_matches:
                    if match.startswith('http'):
                        pdf_links.append({"url": match, "title": PDFExtractor._extract_filename(match)})
                    elif match.startswith('/'):
                        full_url = urljoin(page_url, match)
                        pdf_links.append({"url": full_url, "title": PDFExtractor._extract_filename(full_url)})
            
            # å»é‡
            seen = set()
            unique_links = []
            for link in pdf_links:
                if link['url'] not in seen:
                    seen.add(link['url'])
                    unique_links.append(link)
            
            return unique_links
            
        except Exception as e:
            print(f"âŒ æå– PDF é“¾æ¥å¤±è´¥: {e}")
            return []
    
    @staticmethod
    def download_and_parse_pdf(pdf_url: str, max_pages: int = 10) -> Tuple[str, Optional[str]]:
        """
        ä¸‹è½½ PDF å¹¶æå–å…¨æ–‡å†…å®¹ (é™åˆ¶å‰10é¡µä»¥å¹³è¡¡æ€§èƒ½ä¸èµ„æº)
        
        Returns:
            (æå– of çš„æ–‡æœ¬å†…å®¹, é”™è¯¯ä¿¡æ¯æˆ–None)
        """
        if not HAS_PYMUPDF:
            return "", "PyMuPDF æœªå®‰è£…"
        
        try:
            print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ PDF: {pdf_url[:80]}...")
            # å¢åŠ è¶…æ—¶ä¿æŠ¤
            response = requests.get(pdf_url, headers=PDFExtractor.HEADERS, timeout=20, verify=False)
            response.raise_for_status()
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å° (å¦‚è¶…è¿‡ 15MB åˆ™è·³è¿‡ä¸‹è½½ï¼Œé¿å…å†…å­˜å´©æºƒ)
            file_size = len(response.content)
            if file_size > 15 * 1024 * 1024:
                return "", f"æ–‡ä»¶è¿‡å¤§ ({file_size / 1024 / 1024:.1f}MB)ï¼Œè·³è¿‡æ·±åº¦ä¸‹è½½ä»¥èŠ‚çœèµ„æº"
            
            # ä½¿ç”¨ PyMuPDF è§£æ
            doc = fitz.open(stream=response.content, filetype="pdf")
            
            text_parts = []
            # é™åˆ¶é¡µæ•°ï¼Œæ”¿ç­–æ–‡ä»¶æ ¸å¿ƒé€šå¸¸åœ¨å‰10é¡µ
            page_count = min(len(doc), max_pages)
            
            for page_num in range(page_count):
                page = doc[page_num]
                page_text = page.get_text("text")
                if page_text.strip():
                    text_parts.append(f"--- ç¬¬ {page_num + 1} é¡µ ---\n{page_text}")
            
            doc.close()
            
            full_text = "\n\n".join(text_parts)
            # é™åˆ¶æ€»å­—ç¬¦ï¼Œé¿å… token çˆ†ç‚¸
            full_text = full_text[:30000]
            
            print(f"âœ… PDF è§£æå®Œæˆ: {len(full_text)} å­—ç¬¦, {page_count} é¡µ")
            
            return full_text, None
            
        except requests.exceptions.RequestException as e:
            return "", f"ä¸‹è½½å¤±è´¥: {e}"
        except Exception as e:
            return "", f"è§£æå¤±è´¥: {e}"
    
    @staticmethod
    def extract_and_parse(page_url: str, html_content: Optional[str] = None) -> Dict[str, any]:
        """
        ä¸€ç«™å¼æå–ï¼šä»ç½‘é¡µæå– PDF é“¾æ¥å¹¶è§£æå†…å®¹
        
        Returns:
            {
                "pdf_links": [{"url": "...", "title": "..."}],
                "pdf_content": "è§£æå‡ºçš„PDFå…¨æ–‡",
                "source_pdf_url": "å†…å®¹æ¥æºçš„PDFé“¾æ¥",
                "error": "é”™è¯¯ä¿¡æ¯æˆ–None"
            }
        """
        result = {
            "pdf_links": [],
            "pdf_content": "",
            "source_pdf_url": None,
            "error": None
        }
        
        # 1. æå–æ‰€æœ‰ PDF é“¾æ¥
        pdf_links = PDFExtractor.extract_pdf_links(page_url, html_content)
        result["pdf_links"] = pdf_links
        
        if not pdf_links:
            result["error"] = "æœªåœ¨ç½‘é¡µä¸­å‘ç° PDF æ–‡ä»¶"
            return result
        
        # 2. å°è¯•è§£æç¬¬ä¸€ä¸ª PDF (é€šå¸¸æ˜¯æ­£æ–‡)
        for link in pdf_links:
            content, error = PDFExtractor.download_and_parse_pdf(link["url"])
            if content and len(content) > 500:  # æœ‰å®è´¨å†…å®¹
                result["pdf_content"] = content
                result["source_pdf_url"] = link["url"]
                break
            elif error:
                result["error"] = error
        
        return result
    
    @staticmethod
    def _extract_filename(url: str) -> str:
        """ä» URL ä¸­æå–æ–‡ä»¶å"""
        parsed = urlparse(url)
        path = parsed.path
        filename = path.split('/')[-1] if '/' in path else path
        return filename or "æœªçŸ¥æ–‡ä»¶"


# å•ä¾‹å®ä¾‹
pdf_extractor = PDFExtractor()


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    test_url = "https://www.csrc.gov.cn/csrc/c100028/c7443184/content.shtml"
    
    print("=== æµ‹è¯• PDF æå– ===")
    result = pdf_extractor.extract_and_parse(test_url)
    
    print(f"\næ‰¾åˆ° PDF é“¾æ¥: {len(result['pdf_links'])}")
    for link in result['pdf_links']:
        print(f"  - {link['title']}: {link['url'][:60]}...")
    
    if result['pdf_content']:
        print(f"\nPDF å†…å®¹é¢„è§ˆ (å‰500å­—):\n{result['pdf_content'][:500]}")
    else:
        print(f"\né”™è¯¯: {result['error']}")
