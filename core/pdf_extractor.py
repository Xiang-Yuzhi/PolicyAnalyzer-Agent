"""
PDF Extractor: ä»ç½‘é¡µä¸­æå–å¹¶è§£æ PDF æ–‡ä»¶

åŠŸèƒ½ï¼š
1. ä»ç½‘é¡µå†…å®¹ä¸­æå– PDF ä¸‹è½½é“¾æ¥
2. ä¸‹è½½ PDF æ–‡ä»¶å¹¶æå–å…¨æ–‡å†…å®¹
3. æ”¯æŒåµŒå…¥å¼ PDF (embed/iframe) å’Œç›´æ¥é“¾æ¥
"""

import re
import requests
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, urlparse

# PDF è§£æ
try:
    import fitz  # PyMuPDF
    HAS_PYMUPDF = True
except ImportError:
    HAS_PYMUPDF = False
    print("âš ï¸ PyMuPDF æœªå®‰è£…ï¼Œå°†æ— æ³•è§£æ PDF å†…å®¹")

# HTML è§£æ
try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False
    print("âš ï¸ BeautifulSoup æœªå®‰è£…ï¼Œå°†æ— æ³•æå– PDF é“¾æ¥")


class PDFExtractor:
    """PDF æå–ä¸è§£æå™¨"""
    
    # å¸¸è§çš„æ”¿ç­– PDF æ–‡ä»¶åæ¨¡å¼
    PDF_PATTERNS = [
        r'\.pdf$',
        r'\.PDF$',
        r'/download/',
        r'/attachment/',
        r'/file/',
    ]
    
    # è¯·æ±‚å¤´æ¨¡æ‹Ÿæµè§ˆå™¨
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    }
    
    @staticmethod
    def extract_pdf_links(page_url: str, html_content: Optional[str] = None) -> List[Dict[str, str]]:
        """
        ä»ç½‘é¡µä¸­æå– PDF ä¸‹è½½é“¾æ¥ï¼ˆä¼˜åŒ–è¯ç›‘ä¼šç­‰æ”¿åºœç½‘ç«™ï¼‰
        
        Returns:
            [{"url": "å®Œæ•´PDFé“¾æ¥", "title": "é“¾æ¥æ–‡æœ¬æˆ–æ–‡ä»¶å"}]
        """
        if not HAS_BS4:
            return []
        
        pdf_links = []
        
        try:
            if not html_content:
                response = requests.get(page_url, headers=PDFExtractor.HEADERS, timeout=15, verify=False)
                response.encoding = response.apparent_encoding  # ä¿®å¤ç¼–ç é—®é¢˜
                html_content = response.text
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # è§£æåŸºç¡€ URLï¼ˆç”¨äºæ­£ç¡®æ‹¼æ¥ç›¸å¯¹è·¯å¾„ï¼‰
            parsed_base = urlparse(page_url)
            base_url = f"{parsed_base.scheme}://{parsed_base.netloc}"
            # è·å–å½“å‰é¡µé¢æ‰€åœ¨ç›®å½•ï¼ˆç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
            page_dir = page_url.rsplit('/', 1)[0] if '/' in parsed_base.path else page_url
            
            print(f"ğŸ” æ­£åœ¨åˆ†æé¡µé¢: {page_url}")
            print(f"   åŸºç¡€URL: {base_url}, é¡µé¢ç›®å½•: {page_dir}")
            
            # ä¼˜å…ˆæ–¹å¼: ä» #files æˆ– class=files å®¹å™¨ä¸­æå– (è¯ç›‘ä¼šç½‘ç«™ç‰¹æœ‰ç»“æ„)
            files_containers = soup.find_all(['div', 'section'], id='files') or \
                               soup.find_all(['div', 'section'], class_='files') or \
                               soup.find_all(['div'], id=re.compile(r'file', re.I))
            
            if files_containers:
                print(f"   âœ… æ‰¾åˆ° {len(files_containers)} ä¸ªæ–‡ä»¶å®¹å™¨")
                for container in files_containers:
                    for a in container.find_all('a', href=True):
                        href = a.get('href', '')
                        if re.search(r'\.pdf', href, re.I):
                            # æ™ºèƒ½æ‹¼æ¥å®Œæ•´ URL
                            full_url = PDFExtractor._build_full_url(href, page_url, base_url, page_dir)
                            title = a.get_text(strip=True) or PDFExtractor._extract_filename(full_url)
                            pdf_links.append({"url": full_url, "title": title, "source": "files_container"})
                            print(f"   ğŸ“ [å®¹å™¨] {title[:40]} -> {full_url}")
            
            # å¤‡ç”¨æ–¹å¼: å…¨å±€æœç´¢ <a href="xxx.pdf">
            if not pdf_links:
                print(f"   âš ï¸ æœªåœ¨ files å®¹å™¨ä¸­æ‰¾åˆ° PDFï¼Œå°è¯•å…¨å±€æœç´¢")
                for a in soup.find_all('a', href=True):
                    href = a.get('href', '')
                    if re.search(r'\.pdf', href, re.I):
                        full_url = PDFExtractor._build_full_url(href, page_url, base_url, page_dir)
                        title = a.get_text(strip=True) or PDFExtractor._extract_filename(full_url)
                        pdf_links.append({"url": full_url, "title": title, "source": "global_search"})
                        print(f"   ğŸ“ [å…¨å±€] {title[:40]} -> {full_url}")
            
            # å»é‡
            seen = set()
            unique_links = []
            for link in pdf_links:
                if link['url'] not in seen:
                    seen.add(link['url'])
                    unique_links.append(link)
            
            print(f"   ğŸ“Š å…±æ‰¾åˆ° {len(unique_links)} ä¸ªå”¯ä¸€ PDF é“¾æ¥")
            return unique_links
            
        except Exception as e:
            print(f"âŒ æå– PDF é“¾æ¥å¤±è´¥: {e}")
            return []
    
    @staticmethod
    def _build_full_url(href: str, page_url: str, base_url: str, page_dir: str) -> str:
        """æ™ºèƒ½æ‹¼æ¥å®Œæ•´ URL"""
        href = href.strip()
        
        # å·²ç»æ˜¯å®Œæ•´ URL
        if href.startswith('http://') or href.startswith('https://'):
            return href
        
        # ç»å¯¹è·¯å¾„ (ä»¥ / å¼€å¤´)
        if href.startswith('/'):
            return base_url + href
        
        # ç›¸å¯¹è·¯å¾„ (ä¸ä»¥ / å¼€å¤´)
        # ä½¿ç”¨é¡µé¢æ‰€åœ¨ç›®å½•æ‹¼æ¥
        return page_dir + '/' + href
    
    @staticmethod
    def download_and_parse_pdf(pdf_url: str, max_pages: int = 15) -> Tuple[str, Optional[str]]:
        """
        ä¸‹è½½ PDF å¹¶æå–å…¨æ–‡å†…å®¹
        
        Returns:
            (æå–çš„æ–‡æœ¬å†…å®¹, é”™è¯¯ä¿¡æ¯æˆ–None)
        """
        if not HAS_PYMUPDF:
            return "", "PyMuPDF æœªå®‰è£…"
        
        try:
            print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ PDF: {pdf_url}")
            
            # å¢åŠ é‡å®šå‘è·Ÿè¸ªï¼Œä½¿ç”¨ Session ä¿æŒ cookies
            session = requests.Session()
            response = session.get(
                pdf_url, 
                headers=PDFExtractor.HEADERS, 
                timeout=30, 
                verify=False,
                allow_redirects=True
            )
            response.raise_for_status()
            
            content_type = response.headers.get('Content-Type', 'unknown')
            final_url = response.url  # è·Ÿè¸ªé‡å®šå‘åçš„æœ€ç»ˆ URL
            file_size = len(response.content)
            
            print(f"  ğŸ“Š å“åº”ä¿¡æ¯: çŠ¶æ€ç ={response.status_code}, å†…å®¹ç±»å‹={content_type}, å¤§å°={file_size/1024:.1f}KB")
            print(f"  ğŸ”— æœ€ç»ˆURL: {final_url}")
            
            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°é PDF é¡µé¢
            if final_url != pdf_url:
                print(f"  âš ï¸ å‘ç”Ÿé‡å®šå‘: {pdf_url} -> {final_url}")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            if file_size > 15 * 1024 * 1024:
                return "", f"æ–‡ä»¶è¿‡å¤§ ({file_size / 1024 / 1024:.1f}MB)ï¼Œè·³è¿‡ä¸‹è½½"
            
            if file_size < 100:
                return "", f"æ–‡ä»¶è¿‡å° ({file_size} å­—èŠ‚)ï¼Œå¯èƒ½ä¸æ˜¯æœ‰æ•ˆ PDF"
            
            # æ ¸å¿ƒä¿®å¤ï¼šæ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºæœ‰æ•ˆ PDF (é­”æ•°éªŒè¯)
            pdf_content = response.content
            if not pdf_content.startswith(b'%PDF'):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ HTML é”™è¯¯é¡µ
                if b'<html' in pdf_content[:500].lower() or b'<!doctype' in pdf_content[:500].lower():
                    error_preview = pdf_content[:200].decode('utf-8', errors='ignore')
                    print(f"  âŒ æœåŠ¡å™¨è¿”å› HTML è€Œé PDF: {error_preview[:100]}...")
                    return "", "æœåŠ¡å™¨è¿”å› HTML é¡µé¢è€Œé PDF æ–‡ä»¶ï¼ˆå¯èƒ½éœ€è¦ç™»å½•æˆ–é“¾æ¥å·²å¤±æ•ˆï¼‰"
                else:
                    print(f"  âŒ å†…å®¹ä¸æ˜¯æœ‰æ•ˆ PDFï¼Œå‰20å­—èŠ‚: {pdf_content[:20]}")
                    return "", "ä¸‹è½½çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„ PDF æ–‡ä»¶"
            
            print(f"  âœ… PDF é­”æ•°éªŒè¯é€šè¿‡ï¼Œå¼€å§‹è§£æ...")
            
            # ä½¿ç”¨ PyMuPDF è§£æ
            doc = fitz.open(stream=pdf_content, filetype="pdf")
            
            text_parts = []
            page_count = min(len(doc), max_pages)
            
            for page_num in range(page_count):
                page = doc[page_num]
                page_text = page.get_text("text")
                if page_text.strip():
                    text_parts.append(f"--- ç¬¬ {page_num + 1} é¡µ ---\n{page_text}")
            
            total_pages = len(doc)
            doc.close()
            
            full_text = "\n\n".join(text_parts)
            full_text = full_text[:30000]  # é™åˆ¶æ€»å­—ç¬¦
            
            print(f"  âœ… PDF è§£ææˆåŠŸ: {len(full_text)} å­—ç¬¦, è§£æ {page_count}/{total_pages} é¡µ")
            
            return full_text, None
            
        except requests.exceptions.RequestException as e:
            print(f"  âŒ ä¸‹è½½å¤±è´¥: {e}")
            return "", f"ä¸‹è½½å¤±è´¥: {e}"
        except Exception as e:
            print(f"  âŒ è§£æå¤±è´¥: {e}")
            return "", f"è§£æå¤±è´¥: {e}"
    
    @staticmethod
    def extract_and_parse(page_url: str, html_content: Optional[str] = None) -> Dict[str, any]:
        """
        ä¸€ç«™å¼æå–ï¼šä»ç½‘é¡µæå– PDF é“¾æ¥å¹¶è§£æå†…å®¹
        
        Returns:
            {
                "pdf_links": [{"url": "...", "title": "..."}],
                "pdf_content": "è§£æå‡ºçš„PDFå…¨æ–‡",
                "source_pdf_url": "å†…å®¹æ¥æºçš„PDFé“¾æ¥",
                "error": "é”™è¯¯ä¿¡æ¯æˆ–None"
            }
        """
        result = {
            "pdf_links": [],
            "pdf_content": "",
            "source_pdf_url": None,
            "error": None
        }
        
        # 1. æå–æ‰€æœ‰ PDF é“¾æ¥
        pdf_links = PDFExtractor.extract_pdf_links(page_url, html_content)
        result["pdf_links"] = pdf_links
        print(f"ğŸ” åœ¨é¡µé¢ä¸­å‘ç° {len(pdf_links)} ä¸ªå¯èƒ½çš„ PDF é“¾æ¥")
        
        if not pdf_links:
            result["error"] = "æœªåœ¨ç½‘é¡µä¸­å‘ç° PDF æ–‡ä»¶"
            return result
        
        # 2. å°è¯•è§£æç¬¬ä¸€ä¸ª PDF (é€šå¸¸æ˜¯æ­£æ–‡)
        for link in pdf_links:
            content, error = PDFExtractor.download_and_parse_pdf(link["url"])
            if content and len(content) > 500:  # æœ‰å®è´¨å†…å®¹
                result["pdf_content"] = content
                result["source_pdf_url"] = link["url"]
                break
            elif error:
                result["error"] = error
        
        return result
    
    @staticmethod
    def _extract_filename(url: str) -> str:
        """ä» URL ä¸­æå–æ–‡ä»¶å"""
        parsed = urlparse(url)
        path = parsed.path
        filename = path.split('/')[-1] if '/' in path else path
        return filename or "æœªçŸ¥æ–‡ä»¶"


# å•ä¾‹å®ä¾‹
pdf_extractor = PDFExtractor()


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    test_url = "https://www.csrc.gov.cn/csrc/c100028/c7443184/content.shtml"
    
    print("=== æµ‹è¯• PDF æå– ===")
    result = pdf_extractor.extract_and_parse(test_url)
    
    print(f"\næ‰¾åˆ° PDF é“¾æ¥: {len(result['pdf_links'])}")
    for link in result['pdf_links']:
        print(f"  - {link['title']}: {link['url'][:60]}...")
    
    if result['pdf_content']:
        print(f"\nPDF å†…å®¹é¢„è§ˆ (å‰500å­—):\n{result['pdf_content'][:500]}")
    else:
        print(f"\né”™è¯¯: {result['error']}")
